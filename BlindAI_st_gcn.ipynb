{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HussamSoufi/googleColab/blob/main/BlindAI_st_gcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r0Yq4vId0vl"
      },
      "source": [
        "# Example of BlindAI deployment with st-gcn\n",
        "\n",
        "We will see in this notebook how to deploy and query a st-gcn model on BlindAI Cloud. To upload of the model, make sure you have an API key.\n",
        "\n",
        "You can get one on the [Mithril Cloud](https://cloud.mithrilsecurity.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igMES8h2d0vm"
      },
      "source": [
        "First, let's install `blindai`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VzGppKdFd0vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f79840-10f9-4ec9-ed54-3250e67f08c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: blindai in /usr/local/lib/python3.7/dist-packages (0.5.11)\n",
            "Requirement already satisfied: protobuf==3.20.1 in /usr/local/lib/python3.7/dist-packages (from blindai) (3.20.1)\n",
            "Requirement already satisfied: grpcio-tools==1.48.1 in /usr/local/lib/python3.7/dist-packages (from blindai) (1.48.1)\n",
            "Requirement already satisfied: bitstring in /usr/local/lib/python3.7/dist-packages (from blindai) (3.1.9)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from blindai) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from blindai) (4.1.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from blindai) (38.0.3)\n",
            "Requirement already satisfied: grpcio==1.48.1 in /usr/local/lib/python3.7/dist-packages (from blindai) (1.48.1)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio==1.48.1->blindai) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from grpcio-tools==1.48.1->blindai) (57.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->blindai) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->blindai) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install blindai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq9vuV-Wd0vn"
      },
      "source": [
        "# Uploading the model to BlindAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm9K3LITd0vp"
      },
      "source": [
        "The first step is to get the model in ONNX format. Let's pull the st-gcn model and export in in ONNX."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a fork of https://github.com/yysijie/st-gcn\n",
        "!wget -N --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1EsbOCzY02VZZvGlbCS5E6jpm9TfjUwJp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1EsbOCzY02VZZvGlbCS5E6jpm9TfjUwJp\" -O st-gcn-onnx.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -o st-gcn-onnx.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WTmxTek138t",
        "outputId": "117de266-e28c-4fda-e175-cbe1fcc57ae8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  st-gcn-onnx.zip\n",
            "f1182f4580e173dde4759341d438bad37722ef2d\n",
            "   creating: st-gcn-onnx-master/\n",
            "  inflating: st-gcn-onnx-master/.gitignore  \n",
            "  inflating: st-gcn-onnx-master/ISSUE_TEMPLATE.md  \n",
            "  inflating: st-gcn-onnx-master/LICENSE  \n",
            "  inflating: st-gcn-onnx-master/OLD_README.md  \n",
            "  inflating: st-gcn-onnx-master/README.md  \n",
            "   creating: st-gcn-onnx-master/config/\n",
            "   creating: st-gcn-onnx-master/config/st_gcn.twostream/\n",
            "   creating: st-gcn-onnx-master/config/st_gcn.twostream/ntu-xsub/\n",
            "  inflating: st-gcn-onnx-master/config/st_gcn.twostream/ntu-xsub/train.yaml  \n",
            "   creating: st-gcn-onnx-master/config/st_gcn.twostream/ntu-xview/\n",
            "  inflating: st-gcn-onnx-master/config/st_gcn.twostream/ntu-xview/train.yaml  \n",
            "   creating: st-gcn-onnx-master/config/st_gcn/\n",
            "   creating: st-gcn-onnx-master/config/st_gcn/kinetics-skeleton/\n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/kinetics-skeleton/demo_offline.yaml  \n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/kinetics-skeleton/demo_old.yaml  \n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/kinetics-skeleton/demo_realtime.yaml  \n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/kinetics-skeleton/test.yaml  \n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/kinetics-skeleton/train.yaml  \n",
            "   creating: st-gcn-onnx-master/config/st_gcn/ntu-xsub/\n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/ntu-xsub/test.yaml  \n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/ntu-xsub/train.yaml  \n",
            "   creating: st-gcn-onnx-master/config/st_gcn/ntu-xview/\n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/ntu-xview/test.yaml  \n",
            "  inflating: st-gcn-onnx-master/config/st_gcn/ntu-xview/train.yaml  \n",
            "  inflating: st-gcn-onnx-master/export.py  \n",
            "   creating: st-gcn-onnx-master/feeder/\n",
            " extracting: st-gcn-onnx-master/feeder/__init__.py  \n",
            "  inflating: st-gcn-onnx-master/feeder/feeder.py  \n",
            "  inflating: st-gcn-onnx-master/feeder/feeder_kinetics.py  \n",
            "  inflating: st-gcn-onnx-master/feeder/tools.py  \n",
            "  inflating: st-gcn-onnx-master/main.py  \n",
            "   creating: st-gcn-onnx-master/models/\n",
            "   creating: st-gcn-onnx-master/models/pose/\n",
            "   creating: st-gcn-onnx-master/models/pose/coco/\n",
            "  inflating: st-gcn-onnx-master/models/pose/coco/pose_deploy_linevec.prototxt  \n",
            "   creating: st-gcn-onnx-master/net/\n",
            " extracting: st-gcn-onnx-master/net/__init__.py  \n",
            "  inflating: st-gcn-onnx-master/net/st_gcn.py  \n",
            "  inflating: st-gcn-onnx-master/net/st_gcn_twostream.py  \n",
            "   creating: st-gcn-onnx-master/net/utils/\n",
            " extracting: st-gcn-onnx-master/net/utils/__init__.py  \n",
            "  inflating: st-gcn-onnx-master/net/utils/graph.py  \n",
            "  inflating: st-gcn-onnx-master/net/utils/tgcn.py  \n",
            "  inflating: st-gcn-onnx-master/notebook.ipynb  \n",
            "   creating: st-gcn-onnx-master/processor/\n",
            " extracting: st-gcn-onnx-master/processor/__init__.py  \n",
            "  inflating: st-gcn-onnx-master/processor/demo_offline.py  \n",
            "  inflating: st-gcn-onnx-master/processor/demo_old.py  \n",
            "  inflating: st-gcn-onnx-master/processor/demo_realtime.py  \n",
            "  inflating: st-gcn-onnx-master/processor/io.py  \n",
            "  inflating: st-gcn-onnx-master/processor/processor.py  \n",
            "  inflating: st-gcn-onnx-master/processor/recognition.py  \n",
            "  inflating: st-gcn-onnx-master/requirements.txt  \n",
            "   creating: st-gcn-onnx-master/resource/\n",
            "   creating: st-gcn-onnx-master/resource/NTU-RGB-D/\n",
            "  inflating: st-gcn-onnx-master/resource/NTU-RGB-D/samples_with_missing_skeletons.txt  \n",
            "   creating: st-gcn-onnx-master/resource/demo_asset/\n",
            "  inflating: st-gcn-onnx-master/resource/demo_asset/attention+prediction.png  \n",
            "  inflating: st-gcn-onnx-master/resource/demo_asset/attention+rgb.png  \n",
            "  inflating: st-gcn-onnx-master/resource/demo_asset/original_video.png  \n",
            "  inflating: st-gcn-onnx-master/resource/demo_asset/pose_estimation.png  \n",
            "   creating: st-gcn-onnx-master/resource/info/\n",
            "  inflating: st-gcn-onnx-master/resource/info/S001C001P001R001A044_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/S001C001P001R001A051_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/S002C001P010R001A017_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/S003C001P008R001A002_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/S003C001P008R001A008_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/clean_and_jerk_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/demo_video.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/hammer_throw_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/juggling_balls_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/pipeline.png  \n",
            "  inflating: st-gcn-onnx-master/resource/info/pull_ups_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/info/tai_chi_w.gif  \n",
            "  inflating: st-gcn-onnx-master/resource/kinetics-motion.txt  \n",
            "   creating: st-gcn-onnx-master/resource/kinetics_skeleton/\n",
            "  inflating: st-gcn-onnx-master/resource/kinetics_skeleton/label_name.txt  \n",
            "   creating: st-gcn-onnx-master/resource/media/\n",
            "  inflating: st-gcn-onnx-master/resource/media/clean_and_jerk.mp4  \n",
            "  inflating: st-gcn-onnx-master/resource/media/skateboarding.mp4  \n",
            "  inflating: st-gcn-onnx-master/resource/media/ta_chi.mp4  \n",
            "  inflating: st-gcn-onnx-master/resource/reference_model.txt  \n",
            "   creating: st-gcn-onnx-master/tools/\n",
            " extracting: st-gcn-onnx-master/tools/__init__.py  \n",
            "  inflating: st-gcn-onnx-master/tools/get_models.sh  \n",
            "  inflating: st-gcn-onnx-master/tools/kinetics_gendata.py  \n",
            "  inflating: st-gcn-onnx-master/tools/ntu_gendata.py  \n",
            "   creating: st-gcn-onnx-master/tools/utils/\n",
            "  inflating: st-gcn-onnx-master/tools/utils/__init__.py  \n",
            "  inflating: st-gcn-onnx-master/tools/utils/ntu_read_skeleton.py  \n",
            "  inflating: st-gcn-onnx-master/tools/utils/openpose.py  \n",
            "  inflating: st-gcn-onnx-master/tools/utils/video.py  \n",
            "  inflating: st-gcn-onnx-master/tools/utils/visualization.py  \n",
            "   creating: st-gcn-onnx-master/torchlight/\n",
            "  inflating: st-gcn-onnx-master/torchlight/setup.py  \n",
            "   creating: st-gcn-onnx-master/torchlight/torchlight/\n",
            "  inflating: st-gcn-onnx-master/torchlight/torchlight/__init__.py  \n",
            "  inflating: st-gcn-onnx-master/torchlight/torchlight/gpu.py  \n",
            "  inflating: st-gcn-onnx-master/torchlight/torchlight/io.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8wmeTdh-d0vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2327f2f-5c35-4e70-8e85-019169707511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/st-gcn-onnx-master/net/utils/tgcn.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert A.size(0) == self.kernel_size\n",
            "/content/st-gcn-onnx-master/net/utils/tgcn.py:63: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  x = x.view(n, self.kernel_size, kc//self.kernel_size, t, v)\n",
            "/content/st-gcn-onnx-master/net/st_gcn.py:87: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  x_shape = [int(s) for s in x.shape[2:]]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/st-gcn-onnx-master')\n",
        "\n",
        "from net.st_gcn import Model\n",
        "import torch\n",
        "\n",
        "mod = Model(in_channels=3, num_class=400, edge_importance_weighting=True, graph_args={\"layout\": \"openpose\", \"strategy\": \"spatial\"})\n",
        "\n",
        "inp = torch.randn(1, 3, 300, 18, 2, dtype=torch.float32)\n",
        "\n",
        "torch.onnx.export(mod, inp, \"st-gcn.onnx\", input_names=[\"input\"], output_names=[\"output\"], opset_version=13)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "inp = torch.from_numpy(np.load(\"./out.npy\"))\n",
        "inp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZJhUdNOMuo6",
        "outputId": "5e300c9f-09dd-4b9e-ef98-bb15eb9b56f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.1280, -0.3470],\n",
              "           [ 0.1120, -0.3610],\n",
              "           [ 0.0950, -0.3610],\n",
              "           ...,\n",
              "           [ 0.1280,  0.0000],\n",
              "           [ 0.1120, -0.3550],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.1260, -0.3530],\n",
              "           [ 0.1120, -0.3610],\n",
              "           [ 0.0950, -0.3630],\n",
              "           ...,\n",
              "           [ 0.1280,  0.0000],\n",
              "           [ 0.1120, -0.3610],\n",
              "           [ 0.1180,  0.0000]],\n",
              "\n",
              "          ...,\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2150,  0.2740],\n",
              "           [-0.1900,  0.2960],\n",
              "           [-0.1880,  0.2960],\n",
              "           ...,\n",
              "           [-0.2250,  0.0000],\n",
              "           [-0.2150,  0.2720],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2120,  0.2720],\n",
              "           [-0.1880,  0.2960],\n",
              "           [-0.1880,  0.2960],\n",
              "           ...,\n",
              "           [-0.2200,  0.0000],\n",
              "           [-0.2120,  0.2720],\n",
              "           [-0.2090,  0.0000]],\n",
              "\n",
              "          ...,\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.6550,  0.3490],\n",
              "           [ 0.9300,  0.7820],\n",
              "           [ 0.8980,  0.8250],\n",
              "           ...,\n",
              "           [ 0.2890,  0.0000],\n",
              "           [ 0.6650,  0.5520],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.6030,  0.1150],\n",
              "           [ 0.9390,  0.4870],\n",
              "           [ 0.8910,  0.4720],\n",
              "           ...,\n",
              "           [ 0.3030,  0.0000],\n",
              "           [ 0.5750,  0.2650],\n",
              "           [ 0.0730,  0.0000]],\n",
              "\n",
              "          ...,\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000]]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7lcAdYEd0vq"
      },
      "source": [
        "Now we can upload the model to BlindAI Cloud. To upload of the model, make sure you have an API key.\n",
        "\n",
        "You can get one on the [Mithril Cloud](https://cloud.mithrilsecurity.io/).\n",
        "\n",
        "You might get an error if the name you want to use is already taken, as models are uniquely identified by their `model_id`. We will implement namespace soon to avoid that. Meanwhile, you will have to choose a unique ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "549mhgnVd0vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b91e4c-1f33-4bbd-c51a-d87225a694d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:blindai.client:Integrity of the model uploaded NOT verified.\n"
          ]
        }
      ],
      "source": [
        "import blindai\n",
        "import uuid\n",
        "\n",
        "api_key = \"7bb41096c82a212815e21bc324f89813\" # Enter your API key here\n",
        "model_id = \"st-gcn-\" + str(uuid.uuid4())\n",
        "\n",
        "# Upload the ONNX file to the remote enclave\n",
        "with blindai.Connection(api_key=api_key) as client:\n",
        "    response = client.upload_model(\"st-gcn.onnx\", model_id=model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOlzSfFGd0vs"
      },
      "source": [
        "## Querying the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eys97sCmd0vv"
      },
      "source": [
        "Now we just need to connect to the model inside the secure enclave and query it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0MWrmaOJd0vv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae0ba19-c539-44f1-81a5-152106861df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "525\n"
          ]
        }
      ],
      "source": [
        "import blindai\n",
        "import torch\n",
        "\n",
        "with blindai.Connection(api_key=api_key) as client:\n",
        "  # Send data to the st-gcn model\n",
        "  prediction = client.predict(model_id, inp)\n",
        "  print(prediction.inference_time)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}